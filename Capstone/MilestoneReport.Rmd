---
title: "Milestone Report for Johns Hopkins Coursera Data Science Capstone"
author: "JBrand"
date: "March 14, 2016"
output: 
  html_document:
    keep_md: true
---

## Introduction
Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. SwiftKey, our corporate partner in this capstone, builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models.  The purpose of this report is to document the beginning of the project, from data import to initial exploratory analysis.

## Data Acquisition, Import, and Sampling

### Acquisition
The first step in the process is to download the file from the weblink:   [https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)
```{r eval=FALSE}
sourcefile <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(sourcefile,"Data/Original/Coursera-SwiftKey.zip")
unzip("Data/Original/Coursera-SwiftKey.zip")
```

### Import
Now that we have the data downloaded and unzipped, we need to import the data into R so we can sample it. In this case, because the files are so large we don't want to do the analysis on the complete dataset because it will require a lot of computing power and time.

```{r cache=TRUE, warning=FALSE}
## First we create connections to the files so we can read them
conTwitter <- file("Data/Original/en_US/en_US.twitter.txt", "r")
conNews <- file("Data/Original/en_US/en_US.news.txt", "r")
conBlogs <- file("Data/Original/en_US/en_US.blogs.txt", "r")

## Now we'll read in the data from those connections
tweets <- readLines(conTwitter, encoding = "UTF-8")
news <- readLines(conNews, encoding = "UTF-8")
blogs <- readLines(conBlogs, encoding = "UTF-8")

## And for tidiness, we'll now close and remove the connections as they're no longer needed
close(conTwitter)
close(conNews)
close(conBlogs)
rm(conTwitter, conNews, conBlogs)

## One final step, to remove all the really weird characters, we'll convert back 
## to ascii and replace strange junk with a space which will get removed later
tweets <- iconv(tweets, "UTF-8", "ascii", sub = " ")
news <- iconv(news, "UTF-8", "ascii", sub = " ")
blogs <- iconv(blogs, "UTF-8", "ascii", sub = " ")
```

### Sampling
It's sample time.  I'm just going to use a random 10% percent of each of the files. And even though I'm setting a seed so it will always sample the same, I'm going to write the samples to sample files so they're always separate and easy to handle.
```{r cache=TRUE}
## Now we sample - I'll use 10% of the lines from each file - which is still conservative.
set.seed(7263)
sampT <- tweets[sample(1:length(tweets),(length(tweets)*.1))]
sampN <- news[sample(1:length(news),(length(news)*.1))]
sampB <- blogs[sample(1:length(blogs),(length(blogs)*.1))]

## Now we'll write to files
writeLines(sampT,"Data/Sampled/sampleTweets.txt")
writeLines(sampN,"Data/Sampled/sampleNews.txt")
writeLines(sampB,"Data/Sampled/sampleBlogs.txt")

## Now that we have everything written to files, we can clear out the memory and make some space.
rm(tweets, news, blogs, sampT, sampN, sampB)
```

## Corpus Creation and Cleaning
```{r message=FALSE}
library(tm)
library(SnowballC)
```
We're going to use the `tm` package for this exercise. So the first thing we need to do is take our new, sampled text files and define a corpus that contains them
```{r cache=TRUE}
myCorpus <- Corpus(DirSource("Data/Sampled"), readerControl=list(reader=readPlain,language="en"))
```
Now that we have a nice clean corpus full of sampled data, we need to clean it up.  It's full of all sorts of things that make natural language processing difficult, i.e.

* capital letters  
* common punctuation  
* numbers  
* English stop words  
* special characters like @ / |  
* whitespace 

Then we'll also want to look at stemming, or the process of reducing inflected words to their stem, or root. An example would be taking "fishing", "fished", and "fisher" and using the root word "fish".

```{r cache=TRUE}
myCorpus <- tm_map(myCorpus, content_transformer(tolower)) ## all lowercase
myCorpus <- tm_map(myCorpus, removePunctuation) ## dump punctuation
myCorpus <- tm_map(myCorpus, removeNumbers) ## dump numbers
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english")) ## dump stopwords

createspace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
myCorpus <- tm_map(myCorpus, createspace, "/|@|\\|") ## function to remove funky characters
rm(createspace)

myCorpus <- tm_map(myCorpus, stripWhitespace) ## remove whitespace
myCorpus <- tm_map(myCorpus, stemDocument, language="english") ## do the stemming
```

## Tokenization
So now that we have a good clean corpus, we can start the tokenization process.  In this case, we want to determine the tokens (words) and also look at some of the N-grams, or ordered sets of words.  We'll look at single words, two-word phrases, and three-word phrases.

```{r cache=TRUE, warning=FALSE}
library(RWeka)

## these commands create the tokenizing functions for each level of N-gram
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))

## these commands actually create the document term matrix for each N-gram
unigramDTM <- DocumentTermMatrix(myCorpus, control = list(tokenize=unigramTokenizer))
bigramDTM <- DocumentTermMatrix(myCorpus, control = list(tokenize=bigramTokenizer))
trigramDTM <- DocumentTermMatrix(myCorpus, control = list(tokenize=trigramTokenizer))
```

## Looking at N-gram Frequencies
So now that we've tokenized for N-grams of length 1, 2, and 3, we can look at the results. First thing we need to do is take our new Document Term Matrices and sort them and pull out the top 25 from each.

```{r cache=TRUE}
uniFreqMatrix <- sort(colSums(as.matrix(unigramDTM)), decreasing=TRUE)
uniFreq <- data.frame(word=names(uniFreqMatrix), freq=uniFreqMatrix, stringsAsFactors = FALSE)
rownames(uniFreq) <- NULL
uniTop25 <- uniFreq[1:25,]
rm(uniFreqMatrix)

biFreqMatrix <- sort(colSums(as.matrix(bigramDTM)), decreasing=TRUE)
biFreq <- data.frame(word=names(biFreqMatrix), freq=biFreqMatrix, stringsAsFactors = FALSE)
rownames(biFreq) <- NULL
biTop25 <- biFreq[1:25,]
rm(biFreqMatrix)

triFreqMatrix <- sort(colSums(as.matrix(trigramDTM)), decreasing=TRUE)
triFreq <- data.frame(word=names(triFreqMatrix), freq=triFreqMatrix, stringsAsFactors = FALSE)
rownames(triFreq) <- NULL
triTop25 <- triFreq[1:25,]
rm(triFreqMatrix)
```
```{r results='asis', cache=TRUE}
library(knitr)
t1 = kable(uniTop25, format = "html", output = F, align = c("l", "c", "c", "c"), escape=F, table.attr="class='table table-striped table-hover'")
t2 = kable(biTop25, format = "html", output = F, align = c("l", "c", "c", "c"), escape=F, table.attr="class='table table-striped table-hover'")
t3 = kable(triTop25, format = "html", output = F, align = c("l", "c", "c", "c"), escape=F, table.attr="class='table table-striped table-hover'")
cat(c('<table><tr><td>"UniGrams"</td><td>', t1, '</td><td>"BiGrams"</td><td>', t2, '</td><td>"TriGrams"</td><td>', t3, '</td><tr></table>'), sep = '')
```
```{r cache=TRUE}
library(ggplot2)
ggplot(uniTop25, aes(x=reorder(word, -freq), y=freq)) + geom_bar(stat="identity") + ggtitle("Top 25 Unigrams") + xlab("Unigrams") + ylab("Frequency") + theme(axis.text.x=element_text(angle=45, hjust=1))

ggplot(biTop25, aes(x=reorder(word, -freq), y=freq)) + geom_bar(stat="identity") + ggtitle("Top 25 Bigrams") + xlab("Bigrams") + ylab("Frequency") + theme(axis.text.x=element_text(angle=45, hjust=1))

ggplot(triTop25, aes(x=reorder(word, -freq), y=freq)) + geom_bar(stat="identity") + ggtitle("Top 25 Trigrams") + xlab("Trigrams") + ylab("Frequency") + theme(axis.text.x=element_text(angle=45, hjust=1))
```

So there's definitely something a bit strange in the bigrams and trigrams since the removal of apostrophes causes the contractions like "don't" to appear as "don" "t" and were tokenized separately. Definitely something I'll need to go back and figure out.  I'm not sure why the stripWhitespace didn't work on those.  But either way, it's a very interesting list.  I'll also need to dig into treating mis-spellings as equivalents - as an example look in the bigrams at "happi birthday".

## Looking Forward
Now, an interesting question that was posed for this project was:  

"How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?"  

Interestingly, we can now use our 1-gram Frequency Data Frame to calculate that with a very simple loop:

```{r cache=TRUE}
## To cover 50% of total word instances based on our sample:
i=0
x=0
while (x < sum(uniFreq[,2])*.5) {
    i <- i +1
    x <- x + uniFreq[i,2]
}
i
```
```{r cache=TRUE}
i=0
x=0
while (x < sum(uniFreq[,2])*.9) {
    i <- i +1
    x <- x + uniFreq[i,2]
}
i
```
To cover 90% of all words used in the sample we took, we only need __7223 words__.  That's really not a lot, and quite impressive.  This will be useful later on as we look at doing the same exercise with our 2- and 3-grams to see how large our codex needs to be for our predictive algorithm.
